{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6808233c",
   "metadata": {},
   "source": [
    "# BERTopic Parameter Tuning and Coherence Analysis\n",
    "\n",
    "This notebook explores a **BERTopic** workflow on the Reddit dataset, performing the following steps:\n",
    "\n",
    "1. **Import** the necessary libraries (including `bertopic`, `sentence_transformers`, `umap`, `hdbscan`, and `gensim`).  \n",
    "2. **Define** helper functions:\n",
    "   - `load_bertopic_docs`: loads processed documents for BERTopic  \n",
    "   - `compute_coherence`: calculates topic coherence  \n",
    "   - `train_and_evaluate_bertopic`: trains and evaluates BERTopic with different parameters  \n",
    "3. **Implement** a `main` function that:\n",
    "   - Loads the data  \n",
    "   - Iterates over various parameters  \n",
    "   - Computes the coherence score  \n",
    "   - Identifies the best combination  \n",
    "   - Prints the highest coherence result  \n",
    "\n",
    "By the end, it becomes clear which **n_neighbors** and **min_cluster_size** yield the highest c_v coherence score for the data, guiding further topic-modelling experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0521e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\anaconda3\\envs\\diss_notebook\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------\n",
    "# 1) Imports and Setup\n",
    "# ----------------------------------------------------------------------------------------\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Gensim imports for coherence\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5222680",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2) Helper Functions\n",
    "\n",
    "These functions handle data loading, coherence measurement, and the actual BERTopic training:\n",
    "\n",
    "- **`load_bertopic_docs(file_path)`**: Gathers `combined_processed` (posts) and `comment_processed` (comments) into a single list of documents for BERTopic.\n",
    "- **`compute_coherence(topic_model, docs)`**: Calculates `c_v` coherence using Gensimâ€™s `CoherenceModel`.\n",
    "- **`train_and_evaluate_bertopic(...)`**: Trains a BERTopic model with specified `n_neighbors` and `min_cluster_size`, updates topics, then computes coherence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c417b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bertopic_docs(file_path: Path) -> list[str]:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_docs = []\n",
    "    for post in data:\n",
    "        main_text = post.get(\"combined_processed\", \"\").strip()\n",
    "        if main_text:\n",
    "            all_docs.append(main_text)\n",
    "\n",
    "        for c in post.get(\"comments\", []):\n",
    "            c_text = c.get(\"comment_processed\", \"\").strip()\n",
    "            if c_text:\n",
    "                all_docs.append(c_text)\n",
    "    return all_docs\n",
    "\n",
    "def compute_coherence(topic_model: BERTopic, docs: list[str]) -> float:\n",
    "    \"\"\"Compute c_v coherence for a BERTopic model using Gensim.\"\"\"\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    # Exclude the outlier topic -1\n",
    "    unique_topics = sorted(t for t in topic_info[\"Topic\"].unique() if t != -1)\n",
    "\n",
    "    topic_words = []\n",
    "    for t in unique_topics:\n",
    "        words_freqs = topic_model.get_topic(t)\n",
    "        words = [w for (w, _) in words_freqs]\n",
    "        topic_words.append(words)\n",
    "\n",
    "    # Prepare texts + dictionary for Gensim\n",
    "    tokenized_docs = [d.split() for d in docs]\n",
    "    dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words,\n",
    "        texts=tokenized_docs,\n",
    "        dictionary=dictionary,\n",
    "        coherence=\"c_v\"\n",
    "    )\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "def train_and_evaluate_bertopic(\n",
    "    docs: list[str],\n",
    "    n_neighbors_val: int,\n",
    "    min_cluster_size_val: int,\n",
    "    verbose: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a BERTopic model with the given parameters and compute c_v coherence.\n",
    "    Returns: (model, coherence_score)\n",
    "    \"\"\"\n",
    "    custom_umap = UMAP(\n",
    "        n_neighbors=n_neighbors_val,\n",
    "        n_components=2,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42,\n",
    "        init=\"random\"\n",
    "    )\n",
    "    custom_hdbscan = HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size_val,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\",\n",
    "        prediction_data=True\n",
    "    )\n",
    "    embedding_model = SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=custom_umap,\n",
    "        hdbscan_model=custom_hdbscan,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    _topics, _probs = topic_model.fit_transform(docs)\n",
    "\n",
    "    # Displayed topic words + bigrams\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "    topic_model.update_topics(docs, vectorizer_model=vectorizer)\n",
    "\n",
    "    coherence_score = compute_coherence(topic_model, docs)\n",
    "    return topic_model, coherence_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec919b5b",
   "metadata": {},
   "source": [
    "## 3) Main Pipeline\n",
    "\n",
    "In the following code cell, the main function is defined and executed to:\n",
    "\n",
    "1. **Load** data from `bertopic_ready_data.json`  \n",
    "2. **Generate** a range of parameter combinations for `n_neighbors` and `min_cluster_size`  \n",
    "3. **Train** a model for each combination, recording coherence scores  \n",
    "4. **Identify** which combination yields the **best** coherence result  \n",
    "5. **Print** the final outcome and provide guidance on future usage of these hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abd0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6479 documents from Data\\bertopic_ready_data.json\n",
      "\n",
      "Trying n_neighbors=5, min_cluster_size=10...\n",
      "  => c_v coherence=0.4061\n",
      "\n",
      "Trying n_neighbors=5, min_cluster_size=15...\n",
      "  => c_v coherence=0.4022\n",
      "\n",
      "Trying n_neighbors=5, min_cluster_size=20...\n",
      "  => c_v coherence=0.3962\n",
      "\n",
      "Trying n_neighbors=10, min_cluster_size=10...\n",
      "  => c_v coherence=0.4463\n",
      "\n",
      "Trying n_neighbors=10, min_cluster_size=15...\n",
      "  => c_v coherence=0.4327\n",
      "\n",
      "Trying n_neighbors=10, min_cluster_size=20...\n",
      "  => c_v coherence=0.4327\n",
      "\n",
      "Trying n_neighbors=15, min_cluster_size=10...\n",
      "  => c_v coherence=0.4571\n",
      "\n",
      "Trying n_neighbors=15, min_cluster_size=15...\n",
      "  => c_v coherence=0.4378\n",
      "\n",
      "Trying n_neighbors=15, min_cluster_size=20...\n",
      "  => c_v coherence=0.4447\n",
      "\n",
      "Best Coherence Score: 0.4571 with n_neighbors=15, min_cluster_size=10\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    input_path = Path(\"Data/bertopic_ready_data.json\")\n",
    "    if not input_path.exists():\n",
    "        print(f\"File {input_path} not found! Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # Load docs\n",
    "    docs = load_bertopic_docs(input_path)\n",
    "    print(f\"Loaded {len(docs)} documents from {input_path}\")\n",
    "\n",
    "    # Parameter search space\n",
    "    n_neighbors_candidates = [5, 10, 15]\n",
    "    min_cluster_size_candidates = [10, 15, 20]\n",
    "\n",
    "    best_model = None\n",
    "    best_score = -1.0\n",
    "    best_params = (None, None)\n",
    "\n",
    "    # Try multiple parameter combos, compute coherence\n",
    "    for nn in n_neighbors_candidates:\n",
    "        for mcs in min_cluster_size_candidates:\n",
    "            print(f\"\\nTrying n_neighbors={nn}, min_cluster_size={mcs}...\")\n",
    "            try:\n",
    "                model, score = train_and_evaluate_bertopic(docs, nn, mcs, verbose=False)\n",
    "                print(f\"  => c_v coherence={score:.4f}\")\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_model = model\n",
    "                    best_params = (nn, mcs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error for n_neighbors={nn}, min_cluster_size={mcs}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "    # If no model succeeded\n",
    "    if not best_model:\n",
    "        print(\"\\nNo successful model found! Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Print best coherence result\n",
    "    print(f\"\\nBest Coherence Score: {best_score:.4f} with n_neighbors={best_params[0]}, min_cluster_size={best_params[1]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d00308",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook explores a range of **n_neighbors** and **min_cluster_size** parameters for **BERTopic**, using the **c_v** coherence metric to assess topic quality. In summary, it:\n",
    "\n",
    "- Loads and combines post/comment texts into a single corpus  \n",
    "- Trains BERTopic models under multiple parameter settings  \n",
    "- Identifies the parameter combination yielding the highest coherence score  \n",
    "\n",
    "These steps enable refinement of the **hyperparameter search**, deeper examination of the **resulting topics**, or further text cleaning and domain-specific stopwords for more nuanced topic structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd95994",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Reference:**  \n",
    "Grootendorst, M. (2022) *BERTopic: Leveraging BERT embeddings for unsupervised topic modeling* [computer program].  \n",
    "Available from: [https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic) [Accessed 12 January 2025].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [BERTopic GitHub](https://github.com/MaartenGr/BERTopic)\n",
    "\n",
    "**Reference:**  \n",
    "Reimers, N., and Gurevych, I. (2019) *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*.  \n",
    "Available from: [https://www.sbert.net/](https://www.sbert.net/) [Accessed 12 January 2025].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [SentenceTransformers GitHub](https://github.com/UKPLab/sentence-transformers)\n",
    "\n",
    "**Reference:**  \n",
    "McInnes, L., Healy, J., and Melville, J. (2018) *UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction* [computer program].  \n",
    "Available from: [https://umap-learn.readthedocs.io/](https://umap-learn.readthedocs.io/) [Accessed 12 January 2025].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [UMAP GitHub](https://github.com/lmcinnes/umap)\n",
    "\n",
    "**Reference:**  \n",
    "Campello, R. J. G. B., Moulavi, D., and Sander, J. (2013) *Density-Based Clustering Based on Hierarchical Density Estimates* [computer program].  \n",
    "Available from: [https://hdbscan.readthedocs.io/](https://hdbscan.readthedocs.io/) [Accessed 12 January 2025].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [HDBSCAN GitHub](https://github.com/scikit-learn-contrib/hdbscan)\n",
    "\n",
    "**Reference:**  \n",
    "Å˜ehÅ¯Å™ek, R., and Sojka, P. (2010) *Software Framework for Topic Modelling with Large Corpora*. In *Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks* [computer program].  \n",
    "Available from: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/) [Accessed 12 January 2025].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Gensim GitHub](https://github.com/RaRe-Technologies/gensim)\n",
    "\n",
    "**Reference:**  \n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011) *Scikit-learn: Machine Learning in Python* [software framework]. *Journal of Machine Learning Research*, 12, pp. 2825â€“2830.  \n",
    "Available from: [https://scikit-learn.org/](https://scikit-learn.org/) [Accessed 12 January 2025].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Scikit-learn GitHub](https://github.com/scikit-learn/scikit-learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_notebook Kernel",
   "language": "python",
   "name": "diss_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
