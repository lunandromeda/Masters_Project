{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b05316-922e-40c3-82fb-39675635a026",
   "metadata": {},
   "source": [
    "# Document Chunking and Minimal Cleaning\n",
    "\n",
    "This notebook performs the following tasks:\n",
    "\n",
    "1. **Import** necessary libraries and set up basic logging.  \n",
    "2. **Define** helper functions to:  \n",
    "   - Clean raw text minimally (removing links, HTML tags, etc.)  \n",
    "   - Optionally *chunk* any document exceeding a specified token threshold into smaller units  \n",
    "3. **Implement** a main pipeline that:  \n",
    "   - Loads the original data (`aggregated_raw_redit_data.json`)  \n",
    "   - Cleans and chunks the data based on parameters (`token_thresh` and `max_sents`)  \n",
    "   - Filters out empty results  \n",
    "   - Saves the output as `bertopic_ready_data.json`\n",
    "\n",
    "This approach ensures that very long posts or comments are not too large for subsequent topic-modelling frameworks like **BERTopic** or **LDA**, while short texts remain unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f2a437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\laure\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------\n",
    "# 1) Imports and Setup\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# File paths for data folder\n",
    "base_folder = r\"C:\\Users\\laure\\Desktop\\dissertation_notebook\"\n",
    "data_folder = os.path.join(base_folder, \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Input/output paths\n",
    "aggregated_path = os.path.join(data_folder, \"aggregated_raw_reddit_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c28a9f",
   "metadata": {},
   "source": [
    "## 2) Helper Functions\n",
    "\n",
    "- **`strip_html_tags(doc)`**: Removes raw HTML tags (e.g., `<p>`, `<div>`).  \n",
    "- **`remove_standalone_links(doc)`**: Strips away standalone links such as `https://something.com`.  \n",
    "- **`minimal_clean(doc)`**: Performs a series of minimal text cleaning steps (e.g., lowercasing, unescaping HTML entities, removing Markdown links).  \n",
    "- **`maybe_chunk_document(doc, max_sents)`**: Splits a document into multiple chunks of up to `max_sents` sentences if chunking is enabled.  \n",
    "- **`process_post(post, max_sents, token_thresh)`**: Applies the above cleaning logic to a single post, including chunking if needed, and processes comments if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4eb8390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(doc: str) -> str:\n",
    "    \"\"\"Remove any <tag> HTML markup.\"\"\"\n",
    "    return re.sub(r\"<[^>]+>\", \"\", doc)\n",
    "\n",
    "def remove_standalone_links(doc: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove raw links,\n",
    "    \"\"\"\n",
    "    link_pattern = r\"(https?://\\S+)|(www\\.\\S+)\"\n",
    "    doc = re.sub(link_pattern, \"\", doc)\n",
    "    return doc\n",
    "\n",
    "def minimal_clean(doc: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform minimal text cleaning:\n",
    "      1. Lowercase text\n",
    "      2. Unescape HTML entities\n",
    "      3. Strip real HTML tags\n",
    "      4. Remove or replace standalone links\n",
    "      5. Remove markdown links [text](url) -> text\n",
    "      6. Remove bold/italic/code markers\n",
    "      7. Remove double quotes\n",
    "      8. Collapse extra spaces\n",
    "    \"\"\"\n",
    "    if not doc:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Lowercase\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # 2) Convert HTML entities\n",
    "    doc = html.unescape(doc)\n",
    "\n",
    "    # 3) Strip real HTML ags\n",
    "    doc = strip_html_tags(doc)\n",
    "\n",
    "    # 4) Remove or replace raw links\n",
    "    doc = remove_standalone_links(doc)\n",
    "\n",
    "    # 5) Remove markdown links: [text](url) -> text\n",
    "    doc = re.sub(r\"\\[([^\\]]+)\\]\\(([^)]+)\\)\", r\"\\1\", doc)\n",
    "\n",
    "    # 6) Remove bold/italic/code markers: **, *, __, _, `\n",
    "    doc = re.sub(r\"(\\*\\*|\\*|__|_|`)\", \"\", doc)\n",
    "\n",
    "    # 7) Remove double quotes\n",
    "    doc = doc.replace('\"', \"\")\n",
    "\n",
    "    # 8) Collapse extra spaces\n",
    "    doc = re.sub(r\"\\s+\", \" \", doc).strip()\n",
    "\n",
    "    return doc\n",
    "\n",
    "def maybe_chunk_document(doc: str, max_sents: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a doc into multiple chunks, each up to 'max_sents' sentences,\n",
    "If max_sents <= 0, just return doc as a single item.\n",
    "    \"\"\"\n",
    "    if not doc:\n",
    "        return []\n",
    "\n",
    "    if max_sents <= 0:\n",
    "        return [doc]\n",
    "\n",
    "    sents = sent_tokenize(doc)\n",
    "    chunks = []\n",
    "    for i in range(0, len(sents), max_sents):\n",
    "        snippet = \" \".join(sents[i : i + max_sents]).strip()\n",
    "        if snippet:\n",
    "            chunks.append(snippet)\n",
    "    return chunks\n",
    "\n",
    "def process_post(post: Dict, max_sents: int = 5, token_thresh: int = 200) -> Dict:\n",
    "    \"\"\"\n",
    "    1) Keep metadata: _id, post_id, subreddit, keyword, submission_score.\n",
    "    2) Keep raw fields: title, selftext, comment.\n",
    "    3) If the cleaned main text > token_thresh, chunk into multiple segments,\n",
    "       else keep it as one segment.\n",
    "    4) Same logic for comments if they're also lengthy, if desired.\n",
    "    5) Preserve post_id and comment_ids throughout processing.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------\n",
    "    # 1) Basic metadata\n",
    "    # ---------------------\n",
    "    post_id = post.get(\"post_id\", post.get(\"_id\", \"\"))\n",
    "    if isinstance(post_id, dict) and \"$oid\" in post_id:\n",
    "        post_id = post_id[\"$oid\"]\n",
    "    \n",
    "    if not post_id:\n",
    "        logger.warning(\"Post found without post_id\")\n",
    "\n",
    "    subreddit = post.get(\"subreddit\", \"\")\n",
    "    keyword = post.get(\"keyword\", \"\")\n",
    "    submission_score = post.get(\"submission_score\", None)\n",
    "\n",
    "    # ---------------------\n",
    "    # 2) Raw text fields\n",
    "    # ---------------------\n",
    "    raw_title = post.get(\"title\", \"\") or \"\"\n",
    "    raw_selftext = post.get(\"selftext\", \"\") or \"\"\n",
    "\n",
    "    # Combine them\n",
    "    combined_text = f\"{raw_title} {raw_selftext}\".strip()\n",
    "    # Minimal clean\n",
    "    cleaned_main = minimal_clean(combined_text)\n",
    "    # Count tokens\n",
    "    main_tokens = cleaned_main.split()\n",
    "\n",
    "    # If the doc is \"long\" (above token_thresh), chunk it >.<\n",
    "    if len(main_tokens) > token_thresh and max_sents > 0:\n",
    "        main_chunks = maybe_chunk_document(cleaned_main, max_sents)\n",
    "        combined_processed = main_chunks[0] if main_chunks else \"\"\n",
    "    else:\n",
    "        combined_processed = cleaned_main\n",
    "\n",
    "    # ---------------------\n",
    "    # 3) Process comments\n",
    "    # ---------------------\n",
    "    processed_comments = []\n",
    "    for c in post.get(\"comments\", []):\n",
    "        comment_id = c.get(\"comment_id\", \"\")\n",
    "        if not comment_id:\n",
    "            logger.warning(f\"Comment found without comment_id in post {post_id}\")\n",
    "\n",
    "        c_raw = c.get(\"comment\", \"\") or \"\"\n",
    "        c_clean = minimal_clean(c_raw)\n",
    "        c_tokens = c_clean.split()\n",
    "\n",
    "        if len(c_tokens) > token_thresh and max_sents > 0:\n",
    "            c_chunks = maybe_chunk_document(c_clean, max_sents)\n",
    "            c_processed = c_chunks[0] if c_chunks else \"\"\n",
    "        else:\n",
    "            c_processed = c_clean\n",
    "\n",
    "        processed_comments.append({\n",
    "            \"comment_id\": comment_id,\n",
    "            \"comment\": c_raw,\n",
    "            \"comment_processed\": c_processed\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        # Metadata\n",
    "        \"post_id\": str(post_id),\n",
    "        \"subreddit\": subreddit,\n",
    "        \"keyword\": keyword,\n",
    "        \"submission_score\": submission_score,\n",
    "\n",
    "        # Raw fields\n",
    "        \"title\": raw_title,\n",
    "        \"selftext\": raw_selftext,\n",
    "\n",
    "        # Potentially chunked result\n",
    "        \"combined_processed\": combined_processed,\n",
    "\n",
    "        # Comments with preserved IDs\n",
    "        \"comments\": processed_comments\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0688f9",
   "metadata": {},
   "source": [
    "## 3) Main Pipeline\n",
    "\n",
    "The main function performs the following steps:\n",
    "\n",
    "1. **Loads** documents from `raw_grouped_data.json`  \n",
    "2. **Applies** cleaning and optional chunking  \n",
    "3. **Filters** out empty results  \n",
    "4. **Saves** the final data as `bertopic_ready_data.json`, along with basic length statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73779fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts, comments not counted: 575\n",
      "Average token-length (combined_processed): 110.84\n",
      "Median token-length: 106\n",
      "Min token-length: 4, Max token-length: 298\n",
      "\n",
      "Saved cleaned data to: C:\\Users\\laure\\Desktop\\dissertation_notebook\\Data\\bertopic_ready_data.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Script to chunk only \"long\" docs above `token_thresh`,\n",
    "    keep them as-is if short, and store in a final JSON matching the LDA/BERTopic-like schema.\n",
    "    \"\"\"\n",
    "    input_path = Path(aggregated_path)\n",
    "    output_path = Path(os.path.join(data_folder, \"bertopic_ready_data.json\"))\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #chunk if doc has more than 200 tokens, in chunks of 5 sentences\n",
    "    token_thresh = 200\n",
    "    max_sents = 5\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for post in data:\n",
    "        processed_post = process_post(post, max_sents=max_sents, token_thresh=token_thresh)\n",
    "        processed_data.append(processed_post)\n",
    "\n",
    "    # Filter out if both combined_processed is empty AND all comment_processed are empty\n",
    "    final_list = []\n",
    "    for doc in processed_data:\n",
    "        if doc[\"combined_processed\"].strip():\n",
    "            final_list.append(doc)\n",
    "        else:\n",
    "            any_comment_nonempty = any(c[\"comment_processed\"].strip() for c in doc[\"comments\"])\n",
    "            if any_comment_nonempty:\n",
    "                final_list.append(doc)\n",
    "\n",
    "    processed_data = final_list\n",
    "\n",
    "    # Save\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(processed_data, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Basic doc length stats (just for combined_processed)\n",
    "    doc_lengths = [len(d[\"combined_processed\"].split()) for d in processed_data]\n",
    "    if doc_lengths:\n",
    "        avg_len = sum(doc_lengths) / len(doc_lengths)\n",
    "        sorted_lens = sorted(doc_lengths)\n",
    "        median_len = sorted_lens[len(sorted_lens)//2]\n",
    "        min_len = min(doc_lengths)\n",
    "        max_len = max(doc_lengths)\n",
    "        print(f\"Total posts, comments not counted: {len(doc_lengths)}\")\n",
    "        print(f\"Average token-length (combined_processed): {avg_len:.2f}\")\n",
    "        print(f\"Median token-length: {median_len}\")\n",
    "        print(f\"Min token-length: {min_len}, Max token-length: {max_len}\")\n",
    "    else:\n",
    "        print(\"No documents after filtering!\")\n",
    "\n",
    "    print(f\"\\nSaved cleaned data to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1fd3a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a **minimal** text cleaning strategy—removing links, HTML tags, and Markdown markers—and **optionally** splitting lengthy texts into smaller chunks based on sentence count. These steps ensure that large documents do not exceed certain token limits, thereby improving the manageability of subsequent analyses such as **BERTopic** or **LDA**.\n",
    "\n",
    "Basic statistics on token lengths are also provided to clarify the size distribution of the final documents. The `bertopic_ready_data.json` file can now be used directly in the next notebook or script for advanced topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c73a45",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Reference:**  \n",
    "Bird, S., Klein, E., and Loper, E. (2009) *Natural Language Processing with Python*. O'Reilly Media Inc.  \n",
    "Available from: [https://www.nltk.org/](https://www.nltk.org/) [Accessed 12 January 2025].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [NLTK GitHub](https://github.com/nltk/nltk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_notebook Kernel",
   "language": "python",
   "name": "diss_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
