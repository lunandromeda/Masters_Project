{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb4d7c-78f3-45b7-943a-14c46b65bfb7",
   "metadata": {},
   "source": [
    "# Collect and Aggregate Reddit Posts: Autism & Healthcare Experiences\n",
    "\n",
    "This notebook will:\n",
    "1. **Collect** Reddit posts/comments about autism healthcare using PRAW.\n",
    "2. **Deduplicate** and **clean** the data.\n",
    "3. **Aggregate** comments under their respective posts.\n",
    "4. **Save** the final aggregated JSON file.\n",
    "\n",
    "### Key Libraries:\n",
    "- [PRAW](https:#praw.readthedocs.io/en/latest/) for Reddit API\n",
    "- [tqdm](https:#tqdm.github.io/) for progress bars\n",
    "- [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) for parallelised data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75941121-2b3b-4b02-a1d8-99bdb08a0eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Imports\n",
    "# ------------------------------------------------------------\n",
    "import praw\n",
    "import prawcore\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64106e4-e8cd-41b2-80ef-bc9468f1aba5",
   "metadata": {},
   "source": [
    "## Reddit API Credentials\n",
    "\n",
    "Replace these placeholders with actual:\n",
    "- `client_id`\n",
    "- `client_secret`\n",
    "- `refresh_token`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01299cea-1d01-4a67-96b4-bac287fdb010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit instance created successfully\n"
     ]
    }
   ],
   "source": [
    "client_id = 'YOUR_CLIENT_ID'\n",
    "client_secret = 'YOUR_CLIENT_SECRET'\n",
    "user_agent = 'YOUR_USER_AGENT'\n",
    "refresh_token = 'YOUR_REFRESH_TOKEN'\n",
    "\n",
    "# Initialise PRAW using the refresh token\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    "    refresh_token=refresh_token\n",
    ")\n",
    "\n",
    "print(\"Reddit instance created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23473765-a351-48d4-8c65-0aa14a894b2e",
   "metadata": {},
   "source": [
    "## Subreddit Configuration & Keywords\n",
    "\n",
    "This notebook includes:\n",
    "- A dictionary of subreddits (`subreddits_info`) and their subscriber counts.\n",
    "- A keyword list capturing diverse aspects of autism healthcare.\n",
    "- Reasonably low thresholds for submission/comment scores to broaden the data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e57805de-e3a2-4c11-90e1-ba6f4e8d35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddit info, keywords, and thresholds set.\n"
     ]
    }
   ],
   "source": [
    "# Subreddit subscrier counts & years they went active\n",
    "subreddits_info = {\n",
    "    'autism':           {'subs': 324486, 'year_active': 2019},\n",
    "    'AutismInWomen':    {'subs': 113095, 'year_active': 2022},\n",
    "    'aspergers':        {'subs': 148902, 'year_active': 2019},\n",
    "    'AutisticAdults':   {'subs': 51258,  'year_active': 2022},\n",
    "    'aspiememes':       {'subs': 202954, 'year_active': 2019},\n",
    "    'Autism_Parenting': {'subs': 29028,  'year_active': 2022},\n",
    "    'aspergirls':       {'subs': 80347,  'year_active': 2021},\n",
    "    'AskAutism':        {'subs': 5949,   'year_active': 2021},\n",
    "    'AutisticWithADHD': {'subs': 32842,  'year_active': 2022},\n",
    "    'neurodiversity':   {'subs': 67833,  'year_active': 2021},\n",
    "    'autismUK':         {'subs': 3926,   'year_active': 2019},\n",
    "}\n",
    "\n",
    "# Healthcare and autism-related search keywords\n",
    "keywords = [\n",
    "    \"healthcare\", \"treatment\", \"therapy\",\n",
    "    \"access to care\", \"insurance\", \"coverage\", \"waiting list\",\n",
    "    \"diagnosis\", \"misdiagnosis\", \"quality of service\", \"bad experience\",\n",
    "    \"good experience\", \"stigma\", \"barriers\", \"disparities\", \"lack of resources\",\n",
    "    \"ABA\", \"occupational therapy\", \"speech therapy\", \"behavioural therapy\",\n",
    "    \"mental health\", \"psychiatrist\", \"psychologist\", \"comorbid conditions\",\n",
    "    \"transition planning\", \"IEP\", \"emotional support\", \"inclusion\", \"burnout\",\n",
    "    \"paediatrician\", \"dentist\", \"mental health services\", \"appointment\",\n",
    "    \"prescription\", \"medication\", \"electroshock\", \"waiting room\", \"sensory overload\"\n",
    "    \"hospital\", \"NHS\", \"health inequality\", \"ADHD\", \"masking\", \n",
    "    \"CBT\", \"cognitive behavioural therapy\", \"vaccine\", \"vaccines cause autism\"\n",
    "]\n",
    "\n",
    "# Search parameters\n",
    "time_filter = 'all'\n",
    "sort_orders = ['relevance', 'new', 'top', 'comments']\n",
    "results_per_query = 100\n",
    "target_total_posts = 10000\n",
    "\n",
    "# Score thresholds\n",
    "MIN_SUBMISSION_SCORE = 10\n",
    "MIN_COMMENT_SCORE = 2\n",
    "\n",
    "print(\"Subreddit info, keywords, and thresholds set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ff028-f928-453b-98c2-89c99a072319",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These functions handle Reddit data collection and concurrency with minimal debug output:\n",
    "\n",
    "- **`get_max_results_per_subreddit(subs)`**  \n",
    "  - Determines the maximum number of posts to collect from a subreddit based on its subscriber count.\n",
    "    \n",
    "\n",
    "- **`is_relevant(submission, keyword)`**  \n",
    "  - Checks whether a submission references the given keyword in both the title and selftext (excluding “the good doctor”) and meets a minimum length requirement.\n",
    "    \n",
    "\n",
    "- **`search_subreddit(subreddit_name, keywords, results_per_query, max_results, pbar)`**  \n",
    "  - Searches a single subreddit across multiple keywords and sort orders.\n",
    "  - Updates a **TQDM progress bar** (`pbar`) each time a new post is collected.\n",
    "  - Retries requests up to 5 times on rate limits or network issues.\n",
    "  - Sleeps briefly between search attempts to avoid spamming the API.\n",
    "    \n",
    "\n",
    "- **`search_reddit(subreddits_info, keywords, results_per_query, target_total_posts)`**  \n",
    "  - Uses `ThreadPoolExecutor` for **concurrent** searching of multiple subreddits.\n",
    "  - Displays a **plain-text** TQDM progress bar (no Jupyter widgets) that increments **once per new post**.\n",
    "  - If the final count exceeds `target_total_posts`, it trims the dataset at the end (random sampling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f887eb09-e3a5-4c43-aa35-b5038713bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def get_max_results_per_subreddit(subs):\n",
    "    \"\"\"\n",
    "    Determine the maximum number of results to fetch based on subreddit subscriber count.\n",
    "    Args:\n",
    "        subs (int): Number of subscribers in the subreddit\n",
    "    Returns:\n",
    "        int: Maximum nmber of results to fetch (1000-3000 based on subscriber count)\n",
    "    \"\"\"\n",
    "    if subs > 200000:\n",
    "        return 3000\n",
    "    elif subs > 100000:\n",
    "        return 2000\n",
    "    elif subs > 50000:\n",
    "        return 1500\n",
    "    else:\n",
    "        return 1000\n",
    "\n",
    "def is_relevant(submission, keyword):\n",
    "    \"\"\"\n",
    "    Filter submissions based on content relevance and quality criteria.\n",
    "    Args:\n",
    "        submission: PRAW submission object containing post data\n",
    "        keyword (str): Search term to match in content\n",
    "    Returns:\n",
    "        bool: True if submission meets all relevance criteria:\n",
    "            - Contains keyword in both title and body\n",
    "            - Meets minimum length requirements\n",
    "             Not related to programme \"The Good Doctor\"\n",
    "    \"\"\"\n",
    "    title_lower = submission.title.lower()\n",
    "    body_lower = submission.selftext.lower()\n",
    "\n",
    "    if (\"good doctor\" in title_lower or \"the good doctor\" in title_lower or\n",
    "        \"good doctor\" in body_lower or \"the good doctor\" in body_lower):\n",
    "        return False\n",
    "\n",
    "    title_contains_kw = (keyword.lower() in title_lower)\n",
    "    body_contains_kw  = (keyword.lower() in body_lower)\n",
    "    sufficient_length = (len(submission.title) > 10 and len(submission.selftext) > 50)\n",
    "\n",
    "    return (title_contains_kw and body_contains_kw and sufficient_length)\n",
    "\n",
    "def search_subreddit(subreddit_name, keywords, results_per_query, max_results, pbar):\n",
    "    \"\"\"\n",
    "    Collect and filter relevant posts and comments from a specific subreddit.\n",
    "    Args:\n",
    "        subreddit_name (str): Name of the subreddit to search\n",
    "        keywords (list): List of keywords to search for\n",
    "        results_per_query (int): Number of results to request per API call\n",
    "        max_results (int): Maximum total results to collect for this subreddit\n",
    "        pbar (tqdm): Progress bar object for tracking collection progress\n",
    "    Returns:\n",
    "        list: Collection of dictionaries containing filtered post/comment data\n",
    "    Note:\n",
    "        - Utilises multiple sort orders to maximise result diversity\n",
    "        - Implements rate limiting and error handling\n",
    "        - Updates progress bar for each collected post\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    for keyword in keywords:\n",
    "        for sort_order in sort_orders:\n",
    "            attempts = 0\n",
    "            while attempts < 5:  # Try up to 5 times for each query\n",
    "                try:\n",
    "                    # Fetch submissions matching keyword with specified parameters\n",
    "                    submissions = subreddit.search(\n",
    "                        keyword,\n",
    "                        limit=results_per_query,\n",
    "                        time_filter=time_filter,\n",
    "                        sort=sort_order\n",
    "                    )\n",
    "                    # Process each submission\n",
    "                    for submission in submissions:\n",
    "                        # Filter by score and relevance\n",
    "                        if (submission.score > MIN_SUBMISSION_SCORE \n",
    "                                and is_relevant(submission, keyword)):\n",
    "\n",
    "                            # Convert submission timestamp to datetime\n",
    "                            post_date = datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)\n",
    "                            \n",
    "                            # Load all top-level comments (no nested comments)\n",
    "                            submission.comments.replace_more(limit=0)\n",
    "\n",
    "                            # Process each comment in the submission\n",
    "                            for comment in submission.comments.list():\n",
    "                                # Filter comments by keyword, score, and length\n",
    "                                if (keyword.lower().strip('\"') in comment.body.lower()\n",
    "                                    and comment.score > MIN_COMMENT_SCORE\n",
    "                                    and len(comment.body) > 30):\n",
    "                                    posts.append({\n",
    "                                        'subreddit': subreddit_name,\n",
    "                                        'keyword': keyword,\n",
    "                                        'title': submission.title,\n",
    "                                        'selftext': submission.selftext,\n",
    "                                        'comment': comment.body,\n",
    "                                        'permalink': comment.permalink,\n",
    "                                        'submission_score': submission.score,\n",
    "                                        'comment_score': comment.score,\n",
    "                                        'created_utc': post_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                                    })\n",
    "                                    # Update the progress bar for each new post\n",
    "                                    pbar.update(1)\n",
    "\n",
    "                                    # If hit max for this subreddit, break\n",
    "                                    if len(posts) >= max_results:\n",
    "                                        break\n",
    "\n",
    "                            if len(posts) >= max_results:\n",
    "                                break\n",
    "\n",
    "                    # A small sleep to reduce rate-limit issues\n",
    "                    time.sleep(1)\n",
    "                    break  # break from attempts on sucess\n",
    "\n",
    "                except prawcore.exceptions.RequestException as e:\n",
    "                    attempts += 1\n",
    "                    print(f\"[ERROR] Request failed: {e}\")\n",
    "                    if hasattr(e.response, 'headers') and 'X-Ratelimit-Reset' in e.response.headers:\n",
    "                        wait_time = int(e.response.headers['X-Ratelimit-Reset'])\n",
    "                        print(f\"[RATE LIMIT] Retrying in {wait_time}s...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        fallback_wait = 5\n",
    "                        print(f\"[RATE LIMIT] No header. Sleeping {fallback_wait}s...\")\n",
    "                        time.sleep(fallback_wait)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Unexpected: {e}\")\n",
    "                    break\n",
    "\n",
    "            if len(posts) >= max_results:\n",
    "                break\n",
    "    return posts\n",
    "\n",
    "def search_reddit(subreddits_info, keywords, results_per_query, target_total_posts):\n",
    "    \"\"\"\n",
    "    Coordinate parallel searches across multiple subreddits with progress tracking.\n",
    "    Args:\n",
    "        subreddits_info (dict): Dictionary of subreddit information including subscriber counts\n",
    "        keywords (list): List of keywords to search for\n",
    "        results_per_query (int): Number of results to request per API call\n",
    "        target_total_posts (int): Total number of posts to collect across all subreddits\n",
    "    Returns:\n",
    "        list: Combned collection of post/comment data from all searched subreddits\n",
    "    Note:\n",
    "        - Utilises ThreadPoolExecutor for parallel processing\n",
    "        - Limits concurrent threads to 10 to prevent API rate limits\n",
    "        - Shows progress with TQDM progress bar\n",
    "    \"\"\"\n",
    "    all_posts = []\n",
    "    # ascii=True -> plain-text progress bar\n",
    "    pbar = tqdm(total=target_total_posts, desc=\"Collecting posts\", ascii=True)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_subreddit = {\n",
    "            executor.submit(\n",
    "                search_subreddit,\n",
    "                name,\n",
    "                keywords,\n",
    "                results_per_query,\n",
    "                get_max_results_per_subreddit(info['subs']),\n",
    "                pbar\n",
    "            ): name\n",
    "            for name, info in subreddits_info.items()\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_subreddit):\n",
    "            subreddit_name = future_to_subreddit[future]\n",
    "            try:\n",
    "                # Extend all_posts with what we got from that subreddit\n",
    "                subreddit_posts = future.result()\n",
    "                all_posts.extend(subreddit_posts)\n",
    "\n",
    "                # If reached or exceeded the target, stop\n",
    "                if len(all_posts) >= target_total_posts:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Problem with {subreddit_name}: {e}\")\n",
    "\n",
    "    pbar.close()\n",
    "    return all_posts\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1126d4f-933f-4969-b2b5-59213e3079fb",
   "metadata": {},
   "source": [
    "## Data Collection, Deduplication, & Cleaning\n",
    "\n",
    "1. Collect posts from all subreddits.\n",
    "2. If total exceeds the target, randomly sample.\n",
    "3. Save raw data, then remove duplicates using `(title, comment)` pairs.\n",
    "4. Save JSON with no more duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934bbb76-d83a-43f9-bfd7-7a542bd8a3f1",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting posts:  32%|###################1                                       | 3247/10000 [06:09<16:06,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response[ERROR] Unexpected: received 429 HTTP response\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting posts:  33%|###################1                                       | 3253/10000 [06:20<16:05,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting posts:  97%|#########################################################  | 9665/10000 [16:14<00:47,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response[ERROR] Unexpected: received 429 HTTP response\n",
      "\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting posts: 12904it [26:16,  6.19it/s]                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Unexpected: received 429 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting posts: 12906it [26:18,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n",
      "[ERROR] Unexpected: received 429 HTTP response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting posts: 14512it [33:59,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data saved at: C:\\Users\\laure\\Desktop\\dissertation_notebook\\Data\\reddit_raw_data.json\n",
      "Collected 10000 total records (pre-dedup).\n",
      "Unique submission-comment combos: 5906\n",
      "Cleaned data saved at: C:\\Users\\laure\\Desktop\\dissertation_notebook\\Data\\reddit_raw_data.json\n"
     ]
    }
   ],
   "source": [
    "# Data processing pipeline\n",
    "base_folder = r\"C:\\Users\\laure\\Desktop\\dissertation_notebook\"\n",
    "data_folder = os.path.join(base_folder, \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "raw_json_path = os.path.join(data_folder, \"reddit_raw_data.json\")\n",
    "no_dupes_json_path = raw_json_path\n",
    "aggregated_path = os.path.join(data_folder, \"aggregated_raw_reddit_data.json\")\n",
    "\n",
    "# 1) Collect data\n",
    "posts = search_reddit(subreddits_info, keywords, results_per_query, target_total_posts)\n",
    "\n",
    "# 2) If exceeded target, randmly sample\n",
    "if len(posts) > target_total_posts:\n",
    "    posts = random.sample(posts, target_total_posts)\n",
    "\n",
    "# 3) Save raw data\n",
    "with open(raw_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(posts, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Raw data saved at: {raw_json_path}\")\n",
    "\n",
    "# 4) Remove duplicates\n",
    "unique_posts = []\n",
    "seen_pairs = set()\n",
    "for post in posts:\n",
    "    pair = (post['title'], post['comment'])\n",
    "    if pair not in seen_pairs:\n",
    "        unique_posts.append(post)\n",
    "        seen_pairs.add(pair)\n",
    "\n",
    "print(f\"Collected {len(posts)} total records (pre-dedup).\")\n",
    "print(f\"Unique submission-comment combos: {len(unique_posts)}\")\n",
    "\n",
    "# 5) Save no dupes data\n",
    "with open(no_dupes_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(unique_posts, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Cleaned data saved at: {no_dupes_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375b1d0-e19c-43f8-b7f9-9e1f0f46c60d",
   "metadata": {},
   "source": [
    "## ID Assignment Process\n",
    "\n",
    "Created unique identifiers for Reddit posts and comments:\n",
    "1. **Post IDs**: Format `P_N` where N is incremental\n",
    "2. **Comment IDs**: Format `C_P_N_X` where:\n",
    "    - P_N matches parent post ID\n",
    "    - X is truncated hash of comment text\n",
    "\n",
    "This enables:\n",
    "- Tracking unique posts via titles\n",
    "- Linking comments to parent posts\n",
    "- Consistent reference system for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aadbace-dc7e-4544-b3f4-df1881f0c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing raw data\n",
    "with open(raw_json_path, 'r', encoding='utf-8') as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "# Add IDs to posts and comments\n",
    "post_ids = {}  # Keep track of post IDs asigned\n",
    "\n",
    "# Process each post/comment pair\n",
    "for post in posts:\n",
    "    title = post['title']  # Use title as key to identify unique posts\n",
    "    \n",
    "    # generate a new post ID\n",
    "    if title not in post_ids:\n",
    "        post_ids[title] = f\"P_{len(post_ids) + 1}\"\n",
    "    \n",
    "    # Add IDs to the post\n",
    "    post['post_id'] = post_ids[title]\n",
    "    post['comment_id'] = f\"C_{post['post_id']}_{hash(post['comment']) % 10000}\"  # Add comment ID\n",
    "\n",
    "# Save the updated data back\n",
    "with open(raw_json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(posts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Added IDs to {len(posts)} posts and their comments\")\n",
    "print(f\"Number of unique posts: {len(post_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac72e5",
   "metadata": {},
   "source": [
    "## Aggregation Step\n",
    "\n",
    "Data is aggregated so each post has a list of all associated comments. The result is saved in a new JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8570771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation complete. Output saved to C:\\Users\\laure\\Desktop\\dissertation_notebook\\Data\\aggregated_raw_reddit_data.json\n"
     ]
    }
   ],
   "source": [
    "# 1) Define base folder and create \"Data\" subfolder\n",
    "base_folder = r\"C:\\Users\\laure\\Desktop\\dissertation_notebook\"\n",
    "data_folder = os.path.join(base_folder, \"Data\")\n",
    "\n",
    "# 2) Load the cleaned data\n",
    "with open(no_dupes_json_path, 'r', encoding='utf-8') as file:\n",
    "    cleaned_data = json.load(file)\n",
    "\n",
    "aggregated_data = defaultdict(lambda: {\n",
    "    \"post_id\": \"\",\n",
    "    \"subreddit\": \"\",\n",
    "    \"keyword\": \"\",\n",
    "    \"title\": \"\",\n",
    "    \"selftext\": \"\",\n",
    "    \"submission_score\": 0,\n",
    "    \"comments\": [],\n",
    "    \"permalink\": \"\",\n",
    "    \"created_utc\": \"\"\n",
    "})\n",
    "\n",
    "# 3) Aggregate cleaned data\n",
    "for post in cleaned_data:\n",
    "    utc_time = post['created_utc']\n",
    "    # Only set post details once\n",
    "    if not aggregated_data[utc_time][\"title\"]:\n",
    "        aggregated_data[utc_time].update({\n",
    "            \"post_id\": post['post_id'],  #Include post ID\n",
    "            \"subreddit\": post['subreddit'],\n",
    "            \"keyword\": post['keyword'],\n",
    "            \"title\": post['title'],\n",
    "            \"selftext\": post['selftext'],\n",
    "            \"submission_score\": post['submission_score'],\n",
    "            \"permalink\": post['permalink'],\n",
    "            \"created_utc\": post['created_utc']\n",
    "        })\n",
    "    # Append the comment with its ID\n",
    "    aggregated_data[utc_time][\"comments\"].append({\n",
    "        \"comment_id\": post[\"comment_id\"],  #Include comment ID\n",
    "        \"comment\": post[\"comment\"],\n",
    "        \"comment_score\": post[\"comment_score\"],\n",
    "        \"comment_permalink\": post[\"permalink\"]\n",
    "    })\n",
    "\n",
    "# 4) Convert dict to a list\n",
    "final_result = list(aggregated_data.values())\n",
    "\n",
    "# 5) Save the aggregated data to a file in the Data folder\n",
    "with open(aggregated_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(final_result, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Aggregation complete. Output saved to {aggregated_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488fb291",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook successfully:\n",
    "1. **Collected** Reddit data from 11 autism-related subreddits using PRAW API\n",
    "2. **Filtered** and **cleaned** posts/comments based on healthcare keywords and quality metrics \n",
    "3. **Removed duplicates** and **aggregated** comments under their respective posts\n",
    "4. **Saved** processed data in JSON format for further analysis\n",
    "\n",
    "The resulting dataset provides valuable insights into autism healthcare experiences as discussed on Reddit, ready for further cleaning to operate topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac6eac",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Reference:**  \n",
    "PRAW Development Team (2024) *PRAW: Python Reddit API Wrapper v7.8.1* [computer program].  \n",
    "Available from: [https://praw.readthedocs.io/en/stable/](https://praw.readthedocs.io/en/stable/) [Accessed 6 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [PRAW GitHub](https://github.com/praw-dev/praw)\n",
    "\n",
    "**Reference:**  \n",
    "tqdm contributors (2024) *tqdm: A Fast, Extensible Progress Bar for Python and CLI v4.67.1* [computer program].  \n",
    "Available from: [https://tqdm.github.io/](https://tqdm.github.io/) [Accessed 8 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [tqdm GitHub](https://github.com/tqdm/tqdm)\n",
    "\n",
    "**Reference:**  \n",
    "Hunter, J. D. (2007) *Matplotlib: A 2D Graphics Environment* [computer program]. *Computing in Science & Engineering*, 9(3), pp. 90–95.  \n",
    "Available from: [https://matplotlib.org/](https://matplotlib.org/) [Accessed 6 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Matplotlib GitHub](https://github.com/matplotlib/matplotlib)\n",
    "\n",
    "**Reference:**  \n",
    "Harris, C. R., Millman, K. J., van der Walt, S. J. et al. (2020) *Array programming with NumPy* [computer program]. *Nature*, 585, pp. 357–362.  \n",
    "Available from: [https://numpy.org/](https://numpy.org/) [Accessed 6 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [NumPy GitHub](https://github.com/numpy/numpy)\n",
    "\n",
    "**Reference:**  \n",
    "Pandas Development Team (2024) *pandas: Powerful data structures for data analysis v2.2.3* [computer program].  \n",
    "Available from: [https://pandas.pydata.org/](https://pandas.pydata.org/) [Accessed 11 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Pandas GitHub](https://github.com/pandas-dev/pandas)\n",
    "\n",
    "**Reference:**  \n",
    "Waskom, M. (2024) *seaborn: Statistical data visualization v0.13.2* [computer program].  \n",
    "Available from: [https://joss.theoj.org/papers/10.21105/joss.03021](https://joss.theoj.org/papers/10.21105/joss.03021) [Accessed 11 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Seaborn GitHub](https://github.com/mwaskom/seaborn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_notebook Kernel",
   "language": "python",
   "name": "diss_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
