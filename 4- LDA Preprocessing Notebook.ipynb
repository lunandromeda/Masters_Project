{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f6a061-7cd7-4b6c-abfc-a76ac793f100",
   "metadata": {},
   "source": [
    "# LDA Preprocessing\n",
    "\n",
    "In this notebook, we:\n",
    "1. **Import** needed libraries and define patterns/constants.\n",
    "2. **Define** preprocessing classes and functions (cleaning artifacts, tokenising, bigram building, etc.).\n",
    "3. **Load** raw data and **build** the bigram model.\n",
    "4. **Process** dataset posts (title, selftext, comments) into fields ready for LDA or BERT-based approaches.\n",
    "5. **Output** the final `lda_ready_data.json`.\n",
    "\n",
    "## Key Libraries\n",
    "\n",
    "- **transformers** (Hugging Face for tokenisation):  \n",
    "  [Hugging Face Transformers docs](https://huggingface.co/docs/transformers)\n",
    "\n",
    "- **nltk** (natural language toolkit):  \n",
    "  [NLTK documentation](https://www.nltk.org/)\n",
    "\n",
    "- **gensim** (topic modelling, phrases):  \n",
    "  [Gensim documentation](https://radimrehurek.com/gensim/)\n",
    "\n",
    "## Imports & Setting Up Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "552f8f38-f9fd-43bc-8705-11bd8b83f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully and logging set up.\n"
     ]
    }
   ],
   "source": [
    "import re               # Regular expressions for text patterns\n",
    "import html             # Handling HTML entities\n",
    "import json             # Reading/writing JSON files\n",
    "import unicodedata      # Unicode normalisation\n",
    "import logging          # Logging setup\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import string           # Punctuation constants, string operations\n",
    "import os               # Operating system utilities\n",
    "\n",
    "# Transformers (Hugging Face) for tokenisation\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# NLTK for stopwords, tokenisation, POS tagging, lemmatisation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Gensim for bigram/phrase models (topic modelling)\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# Collections for counters/frequency distributions\n",
    "from collections import Counter\n",
    "\n",
    "# --------------------------------------------\n",
    "# Logging Set Up\n",
    "# --------------------------------------------\n",
    "\n",
    "# Configure basic logging to show warnings and above.\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Indicate successful import and logger setup\n",
    "print(\"Libraries imported successfully and logging set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25784fe1-1341-4287-97d4-f8925c68ae65",
   "metadata": {},
   "source": [
    "## Defining Patterns, Stopwords & Global Variables\n",
    "\n",
    "In this section\n",
    "1. Specify various regular expression (regex) patterns for cleaning and detecting potential artefacts.\n",
    "2. Define both default and custom stopwords, as well as non-relevant bigrams.\n",
    "3. Initialise our lemmatiser, a placeholder for the bigram model, and store a set of part-of-speech tags we allow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "781997db-ccc8-464d-a4ef-a0c8062fbe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern definitions, stopwords, and global variables have been initialised.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------\n",
    "# Pattern Definitions\n",
    "# ----------------------------------------------------------------------------------------\n",
    "ARTIFACT_PATTERNS = {\n",
    "    # Matches HTML entities like &#xAB;, &#123;, etc.\n",
    "    'html_entities': r'&#x?[0-9a-fA-F]+;',\n",
    "    # Removes zero-width characters such as \\u200B, \\uFEFF\n",
    "    'zero_width': r'[\\u200B\\uFEFF]',\n",
    "    # Detects Markdown-style links, e.g. [text](URL)\n",
    "    'markdown_links': r'\\[([^\\]]+)\\]\\(([^)]+)\\)',\n",
    "    # Matches common Markdown formatting symbols (*, **, _, __, `)\n",
    "    'markdown_format': r'(\\*\\*|\\*|__|_|`)',\n",
    "    # Matches quoted lines that often start with \">\"\n",
    "    'quote_blocks': r'>\\s*(.+?)(\\n|$)',\n",
    "    # Matches “smart quotes” and other similar characters\n",
    "    'smart_quotes': r'[’‘´`“”]',\n",
    "    # Consolidates multiple whitespace characters into one\n",
    "    'whitespace': r'\\s+',\n",
    "    # Looks for encoding artefacts such as �\n",
    "    'encoding_artifacts': r'[�]',\n",
    "    # Removes extra block quote symbols (e.g. >, >>, >>>)\n",
    "    'blockquotes_extra': r'^>+\\s*|\\s*>+\\s*',\n",
    "}\n",
    "\n",
    "# Patterns that look suspicious or might need further checks.\n",
    "SUSPICIOUS_PATTERNS = [\n",
    "    r'&#\\w+;',\n",
    "    r'\\[.*?\\]\\(.*?\\)',\n",
    "    r'[“”‘’´`]',\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Stopwords and Custom Stopwords\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# NLTK stopwords for the English language\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# A user-defined set of common or domain-specific words \n",
    "# that we do not want to treat as meaningful.\n",
    "CUSTOM_STOPWORDS = {\n",
    "    'im', 'ive', 'dont', 'like', 'just', 'know', 'thing', 'really', 'get', 'got',\n",
    "    'would', 'could', 'also', 'one', 'even', 'much', 'still', 'thats', 'well',\n",
    "    'cant', 'im', 'thats', 'going', 'make', 'time', 'this', 'http', 'www', 'com',\n",
    "    'good_doctor', 'thank_you', '40_hours', 'does_anyone', 'for_example', 'non_verbal',\n",
    "    # Domain-specific if too frequent/unhelpful:\n",
    "    'autism', 'autistic', 'people', 'child', 'children', 'parent', 'kid', 'kids',\n",
    "    'year', 'years', 'thing',\n",
    "}\n",
    "\n",
    "# Merge the above sets of stopwords\n",
    "STOPWORDS = STOPWORDS.union(CUSTOM_STOPWORDS)\n",
    "\n",
    "# A set of bigrams irrlevant for our analysis\n",
    "NON_RELEVANT_BIGRAMS = {\n",
    "    \"tik_tok\", \"vice_versa\", \"grain_salt\",\n",
    "    \"sliding_scale\", \"advantages_disadvantages\", \"et_al\",\n",
    "    \"blah_blah\", \"mutually_exclusive\", \"daddy_daddy\"\n",
    "}\n",
    "\n",
    "# Incorporate the non-relevant bigrams into the custom stopwords set\n",
    "CUSTOM_STOPWORDS.update(NON_RELEVANT_BIGRAMS)\n",
    "\n",
    "# Rebuild the full STOPWORDS set\n",
    "STOPWORDS = STOPWORDS.union(CUSTOM_STOPWORDS)\n",
    "\n",
    "# Initialises the WordNet Lemmatiser for word normalisation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Placeholder for a bigram model that will be built later using Gensim's Phrases\n",
    "bigram_model = None\n",
    "\n",
    "# A set of allowed part-of-speech tags used for filtering (nouns, adjectives, verbs, etc.)\n",
    "ALLOWED_POS = {\n",
    "    'NN', 'NNS', 'NNP', 'NNPS',\n",
    "    'JJ', 'JJR', 'JJS',\n",
    "    'RB', 'RBR', 'RBS',\n",
    "    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "}\n",
    "\n",
    "# Tracks the distribution of part-of-speech tags encountered\n",
    "pos_distribution = Counter()\n",
    "\n",
    "print(\"Pattern definitions, stopwords, and global variables have been initialised.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6b360-f49c-4dae-9687-fb669a2d3a04",
   "metadata": {},
   "source": [
    "## 2) Utility Functions for Cleaning & Tokenising\n",
    "\n",
    "Below are helper functions used to cleanse text of various artefacts, normalise spacing/punctuation, and perform tokenisation. This includes removal of suspicious patterns, lemmatisation, stopword filtering, and optional bigram detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b437b15-e783-4a93-9e4b-c593d212111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions for cleaning and tokenising have been defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_artefacts(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes or replaces various artefacts (HTML entities, Markdown formatting, zero-width \n",
    "    characters, etc.) from the provided text.\n",
    "    \"\"\"\n",
    "    # Convert HTML entities to their standard characters\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Remove quote block markers (e.g., > Quote)\n",
    "    text = re.sub(ARTIFACT_PATTERNS['quote_blocks'], r'\\1 ', text)\n",
    "    \n",
    "    # Remove HTML entities and zero-width characters\n",
    "    text = re.sub(ARTIFACT_PATTERNS['html_entities'], '', text)\n",
    "    text = re.sub(ARTIFACT_PATTERNS['zero_width'], '', text)\n",
    "    \n",
    "    # Replace Markdown-style links with just the link text\n",
    "    text = re.sub(ARTIFACT_PATTERNS['markdown_links'], r'\\1', text)\n",
    "    \n",
    "    # Remove common Markdown formatting symbols (*, **, _, __, `)\n",
    "    text = re.sub(ARTIFACT_PATTERNS['markdown_format'], '', text)\n",
    "    \n",
    "    # Replace stray encoding character\n",
    "    text = text.replace(\"�\", \"'\")\n",
    "    \n",
    "    # Convert “smart quotes” to normal apostrophes\n",
    "    text = re.sub(ARTIFACT_PATTERNS['smart_quotes'], \"'\", text)\n",
    "    text = text.replace('\"', \"'\")\n",
    "    \n",
    "    # Consolidate multiple whitespace characters into a single space\n",
    "    text = re.sub(ARTIFACT_PATTERNS['whitespace'], ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def check_artefacts(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the text still contains any suspicious patterns.\n",
    "    \"\"\"\n",
    "    # If any pattern is found, we return False\n",
    "    return not any(re.search(pattern, text) for pattern in SUSPICIOUS_PATTERNS)\n",
    "\n",
    "\n",
    "def clean_parenthetical(text: str, acronyms=None) -> str:\n",
    "    \"\"\"\n",
    "    Removes parenthetical content (e.g., '(some text)') unless it contains \n",
    "    a recognised acronym to preserve meaningful information.\n",
    "    \"\"\"\n",
    "    if not acronyms:\n",
    "        # If no acronyms are provided, remove all parenthetical content.\n",
    "        return re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    def contains_acronym(content: str) -> bool:\n",
    "        # Checks if a given snippet contains any of the specified acronyms.\n",
    "        return any(acr in content.upper() for acr in acronyms)\n",
    "\n",
    "    parts = []\n",
    "    idx = 0\n",
    "    # Find all parenthetical substrings\n",
    "    for match in re.finditer(r'\\(([^()]+)\\)', text):\n",
    "        start, end = match.span()\n",
    "        content = match.group(1)\n",
    "        \n",
    "        # Add the segment before the match\n",
    "        parts.append(text[idx:start])\n",
    "        \n",
    "        # If acronym is present, keep the parenthetical segment\n",
    "        if contains_acronym(content):\n",
    "            parts.append(f\"({content})\")\n",
    "        \n",
    "        # Move past this parenthetical\n",
    "        idx = end\n",
    "    \n",
    "    # Add any remaining text after the final parenthetical\n",
    "    parts.append(text[idx:])\n",
    "    \n",
    "    # Rebuild the string and normalise whitespace\n",
    "    final = ' '.join(filter(None, parts))\n",
    "    final = re.sub(r'\\s+', ' ', final)\n",
    "    return final.strip()\n",
    "\n",
    "\n",
    "def minimal_tokenise(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    A simplified tokenisation function for building the bigram model. \n",
    "    Splits text on whitespace, removes punctuation, and filters out stopwords.\n",
    "\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        text = ''\n",
    "    text = text.lower()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    cleaned = []\n",
    "    for token in tokens:\n",
    "        # Strip punctuation from the start/end of the token\n",
    "        t = token.strip(string.punctuation)\n",
    "        \n",
    "        # Check if the token is alphabetic and not in stopwords\n",
    "        if t and t.isalpha() and t not in STOPWORDS:\n",
    "            cleaned.append(t)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def tokenise_lemmatise_stopwords_bigrams(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Full tokenisation pipeline: lowercasing, punctuation removal, stopword filtering, \n",
    "    lemmatisation, optional bigram detection, and final POS-based filtering.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Initial split\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Clean punctuation and normalise case\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = token.lower().strip(string.punctuation)\n",
    "        # Skip empty or purely punctuation tokens\n",
    "        if not token or all(ch in string.punctuation for ch in token):\n",
    "            continue\n",
    "        cleaned_tokens.append(token)\n",
    "\n",
    "    # Lemmatise and filter stopwords\n",
    "    filtered = []\n",
    "    for token in cleaned_tokens:\n",
    "        if token not in STOPWORDS and len(token) > 1:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            if lemma not in STOPWORDS:\n",
    "                filtered.append(lemma)\n",
    "\n",
    "    # Capture the distribution of POS tags\n",
    "    tagged = pos_tag(filtered)\n",
    "    global pos_distribution\n",
    "    pos_distribution.update(pos for _, pos in tagged)\n",
    "\n",
    "    # Keep only allowed POS tags (e.g., nouns, verbs, adjectives, etc.)\n",
    "    pos_filtered_tokens = [word for (word, pos) in tagged if pos in ALLOWED_POS]\n",
    "\n",
    "    # Apply bigram model if it exists\n",
    "    if bigram_model:\n",
    "        bigram_tokens = bigram_model[pos_filtered_tokens]\n",
    "    else:\n",
    "        bigram_tokens = pos_filtered_tokens\n",
    "\n",
    "    # Final pass to remove tokens that contain stopwords or are non-alphabetic\n",
    "    final_tokens = []\n",
    "    for token in bigram_tokens:\n",
    "        if '_' in token:\n",
    "            # For bigrams, split and check each part\n",
    "            parts = token.split('_')\n",
    "            if any(p in STOPWORDS or not p.isalpha() for p in parts):\n",
    "                continue\n",
    "        else:\n",
    "            # For single tokens, check if it's in stopwords or purely non-alpha\n",
    "            if token in STOPWORDS or not token.isalpha():\n",
    "                continue\n",
    "        \n",
    "        final_tokens.append(token)\n",
    "\n",
    "    return ' '.join(final_tokens)\n",
    "\n",
    "\n",
    "def clean_punctuation_spacing(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Tidies up punctuation spacing: condenses multiple periods, fixes spacing around \n",
    "    commas, apostrophes, and similar issues.\n",
    "\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Convert multiple '.' to '...'\n",
    "    text = re.sub(r'\\.{2,}', '...', text)\n",
    "    # Ensure one space after punctuation\n",
    "    text = re.sub(r'\\s*([.,!?;:])\\s*', r'\\1 ', text)\n",
    "    # Insert a space after sentence-ending punctuation if it's missing\n",
    "    text = re.sub(r'([.!?])(?=\\w)', r'\\1 ', text)\n",
    "    # Handle contractions like \"'s\" or \"'t\" by removing extra spaces\n",
    "    text = re.sub(r\"\\s*'\\s*s\\b\", \"'s\", text)\n",
    "    text = re.sub(r\"\\s*'\\s*t\\b\", \"'t\", text)\n",
    "    # Consolidate extra spaces\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def normalise_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ensures consistent single spacing for whitespace in the text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "print(\"Utility functions for cleaning and tokenising have been defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479f56e-d1ba-49ee-81d9-3cdcfe893708",
   "metadata": {},
   "source": [
    "## 3) Preprocessing Configuration & Classes\n",
    "\n",
    "This section defines:\n",
    "1. A data class (`PreprocessingConfig`) containing constraints and acronyms for text processing.\n",
    "2. A text preprocessor class (`TextPreprocessor`) that orchestrates cleaning, tokenisation, acronym handling, bigram application, and truncation.\n",
    "3. A Reddit data processor class (`RedditDataProcessor`) that uses the text preprocessor on Reddit posts, comments, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebe58947-2294-4d20-90b9-a99c4605dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing configuration and classes have been defined.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    \"\"\"\n",
    "    Holds general configuration values for text preprocessing,\n",
    "    such as character/word limits and acronyms to preserve.\n",
    "    \"\"\"\n",
    "    MIN_CHARS: int = 50\n",
    "    MAX_CHARS: int = 50000\n",
    "    MIN_WORDS: int = 10\n",
    "    MIN_SENTENCES: int = 2\n",
    "    MAX_TOKENS: int = 512\n",
    "\n",
    "    # A set of acronymsto preserve inside parentheses or text\n",
    "    ACRONYMS = {\n",
    "        'ABA', 'ASD', 'NHS', 'GP', 'ADHD', 'IEP', 'OT', 'SLP', 'PT',\n",
    "        'CBT', 'DBT', 'PDA', 'SPD', 'DLD', 'AAC', 'PECS'\n",
    "    }\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    A class that handles the main text preprocessing pipeline:\n",
    "    - Cleaning artefacts\n",
    "    - Handling acronyms\n",
    "    - Tokenising and lemmatising\n",
    "    - Applying bigram models\n",
    "    - Truncating text if it exceeds a token limit\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        # Initialise the config and tokenizer\n",
    "        self.config = PreprocessingConfig()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Main entry point for cleaning, acronym handling, tokenisation, bigrams, etc.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Basic checks on length (characters, words, sentences)\n",
    "        if len(text) < self.config.MIN_CHARS or len(text) > self.config.MAX_CHARS:\n",
    "            return \"\"\n",
    "        if len(text.split()) < self.config.MIN_WORDS:\n",
    "            return \"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        if len([s for s in sentences if s.strip()]) < self.config.MIN_SENTENCES:\n",
    "            return \"\"\n",
    "\n",
    "        # Normalise whitespace\n",
    "        text = normalise_whitespace(text)\n",
    "        \n",
    "        # Clean pipeline-specific artefacts\n",
    "        text = self._clean_artifacts(text)\n",
    "        \n",
    "        # Remove certain parenthetical content unless acronyms are detected\n",
    "        text = clean_parenthetical(text, acronyms=self.config.ACRONYMS)\n",
    "        \n",
    "        # Perform tokenisation, lemmatisation, stopword filtering, and bigram handling\n",
    "        text = tokenise_lemmatise_stopwords_bigrams(text)\n",
    "        \n",
    "        # Tidy punctuation spacing one more time\n",
    "        text = clean_punctuation_spacing(text)\n",
    "        \n",
    "        # Normalise whitespace again in case of new changes\n",
    "        text = normalise_whitespace(text)\n",
    "\n",
    "        # Check if any suspicious patterns remain\n",
    "        if not check_artefacts(text):\n",
    "            self.logger.warning(\"Remaining artefacts detected in processed text: %s\", text[:200])\n",
    "\n",
    "        # Truncate tokens if exceeding maximum allowed\n",
    "        text = self._truncate_tokens(text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def _clean_artifacts(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Additional cleaning of artefacts, such as block quotes, zero-width characters,\n",
    "        HTML entities, Markdown links, etc.\n",
    "        \"\"\"\n",
    "        # Convert to a standard Unicode form\n",
    "        text = unicodedata.normalize('NFKC', text)\n",
    "        \n",
    "        # Replace stray encoding characters\n",
    "        text = text.replace('�', \"'\")\n",
    "        \n",
    "        # Unescape any remaining HTML entities\n",
    "        text = html.unescape(text)\n",
    "\n",
    "        # Remove or substitute patterns as defined in the global ARTIFACT_PATTERNS\n",
    "        text = re.sub(ARTIFACT_PATTERNS['quote_blocks'], r'\\1', text, flags=re.MULTILINE)\n",
    "        text = re.sub(ARTIFACT_PATTERNS['blockquotes_extra'], '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(ARTIFACT_PATTERNS['zero_width'], '', text)\n",
    "        text = re.sub(ARTIFACT_PATTERNS['html_entities'], '', text)\n",
    "        text = re.sub(ARTIFACT_PATTERNS['markdown_links'], r'\\1', text)\n",
    "        text = re.sub(ARTIFACT_PATTERNS['markdown_format'], '', text)\n",
    "\n",
    "        # Replace remaining “smart quotes” with normal apostrophes\n",
    "        text = re.sub(r\"[“”‘’´`]\", \"'\", text)\n",
    "        text = text.replace('\"', \"'\")\n",
    "\n",
    "        # Consolidate multiple whitespace occurrences\n",
    "        text = re.sub(ARTIFACT_PATTERNS['whitespace'], ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _truncate_tokens(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Checks the token length using the model's tokenizer. If it exceeds MAX_TOKENS,\n",
    "        truncate and optionally warn the user up to 5 times.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Encode the text for the model\n",
    "        encoded = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        if len(encoded) > self.config.MAX_TOKENS:\n",
    "            # Check or create a counter attribute on the TextPreprocessor instance\n",
    "            if not hasattr(self, \"_trunc_warn_count\"):\n",
    "                self._trunc_warn_count = 0\n",
    "\n",
    "            # Log a warning only if 5 warnings\n",
    "            if self._trunc_warn_count < 5:\n",
    "                self.logger.warning(\n",
    "                    f\"Text exceeded max tokens ({len(encoded)} > {self.config.MAX_TOKENS}). Truncating...\"\n",
    "                )\n",
    "                self._trunc_warn_count += 1\n",
    "\n",
    "            # Keep space for special tokens (e.g., CLS, SEP)\n",
    "            max_len = self.config.MAX_TOKENS - 2\n",
    "            truncated_enc = encoded[:max_len]\n",
    "            truncated = self.tokenizer.decode(\n",
    "                truncated_enc,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False\n",
    "            )\n",
    "\n",
    "            # Attempt to end on a sentence boundary for smooth truncation\n",
    "            sents = re.split(r'(?<=[.!?]) +', truncated)\n",
    "            if len(sents) > 1:\n",
    "                truncated = ' '.join(sents[:-1])\n",
    "            return truncated.strip()\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "\n",
    "class RedditDataProcessor:\n",
    "    \"\"\"\n",
    "    A class that processes Reddit-like data structures (posts, comments), \n",
    "    leveraging TextPreprocessor for thorough text cleaning and tokenisation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        # Instantiate a TextPreprocessor with a specified model name\n",
    "        self.text_processor = TextPreprocessor(model_name=model_name)\n",
    "\n",
    "    def process_post(self, post: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single post to produce processed versions of title, selftext, and combined text,\n",
    "        and apply the same to each comment if present. Preserves post_id and comment_id fields.\n",
    "        \"\"\"\n",
    "        processed = post.copy()\n",
    "\n",
    "        # Preserve the post_id\n",
    "        post_id = processed.get('post_id', '')\n",
    "        if not post_id:\n",
    "            logger.warning(\"Post found without post_id\")\n",
    "\n",
    "        raw_title = processed.get('title', '')\n",
    "        raw_selftext = processed.get('selftext', '')\n",
    "\n",
    "        # Preprocess title, selftext, and a combined version\n",
    "        processed_title = self.text_processor.preprocess_text(raw_title)\n",
    "        processed_selftext = self.text_processor.preprocess_text(raw_selftext)\n",
    "        combined_raw = f\"{raw_title} {raw_selftext}\"\n",
    "        processed_combined = self.text_processor.preprocess_text(combined_raw)\n",
    "\n",
    "        processed['post_id'] = post_id  # Ensure post_id is preserved\n",
    "        processed['title_processed'] = processed_title\n",
    "        processed['selftext_processed'] = processed_selftext\n",
    "        processed['combined_processed'] = processed_combined\n",
    "\n",
    "        # If comments exist, process each one while preserving comment_ids\n",
    "        if 'comments' in processed and isinstance(processed['comments'], list):\n",
    "            for comment_dict in processed['comments']:\n",
    "                # Preserve the comment_id\n",
    "                comment_id = comment_dict.get('comment_id', '')\n",
    "                if not comment_id:\n",
    "                    logger.warning(f\"Comment found without comment_id in post {post_id}\")\n",
    "                \n",
    "                raw_comment = comment_dict.get('comment', '')\n",
    "                comment_dict['comment_id'] = comment_id  # Ensure comment_id is preserved\n",
    "                comment_dict['comment_processed'] = self.text_processor.preprocess_text(raw_comment)\n",
    "\n",
    "        return processed\n",
    "\n",
    "    def process_dataset(self, data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Applies process_post to each element in a dataset (list of dictionaries).\n",
    "        \"\"\"\n",
    "        return [self.process_post(post) for post in data]\n",
    "\n",
    "print(\"Preprocessing configuration and classes have been defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402502b-aa93-4814-b8f3-b0bffa18c53e",
   "metadata": {},
   "source": [
    "## Main Function: Build Bigram Model, Process Data, and Save Output\n",
    "\n",
    "This section orchestrates the entire flow:\n",
    "1. Loads the aggregated raw Reddit data from JSON.\n",
    "2. Builds a bigram model from minimal-tokenised text.\n",
    "3. Processes the dataset using the `RedditDataProcessor`.\n",
    "4. Prints statistics (top bigrams, top POS tags).\n",
    "5. Saves the processed data to a final JSON output file (`lda_ready_data.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "733a3cb4-3ca5-4de0-a8f3-159844ff7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Bigrams:\n",
      "1. tik_tok - Score: 12281.454545454546\n",
      "2. vice_versa - Score: 11258.0\n",
      "3. grain_salt - Score: 8405.973333333333\n",
      "4. facial_expressions - Score: 6754.8\n",
      "5. sqs_informant - Score: 6433.142857142857\n",
      "6. sliding_scale - Score: 6433.142857142857\n",
      "7. advantages_disadvantages - Score: 6332.625\n",
      "8. et_al - Score: 6259.274131274131\n",
      "9. steph_jones - Score: 5514.122448979591\n",
      "10. suicidal_ideation - Score: 5081.9811912225705\n",
      "11. pros_cons - Score: 4946.306636155607\n",
      "12. rabbit_hole - Score: 4912.581818181818\n",
      "13. trial_error - Score: 4890.352941176471\n",
      "14. status_quo - Score: 4824.857142857142\n",
      "15. blah_blah - Score: 4796.307692307692\n",
      "16. cancelling_headphones - Score: 4658.482758620689\n",
      "17. mutually_exclusive - Score: 4503.2\n",
      "18. daddy_daddy - Score: 4221.75\n",
      "19. operant_conditioning - Score: 4045.84375\n",
      "20. political_climate - Score: 3752.6666666666665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "WARNING:__main__:Text exceeded max tokens (553 > 512). Truncating...\n",
      "WARNING:__main__:Text exceeded max tokens (558 > 512). Truncating...\n",
      "WARNING:__main__:Text exceeded max tokens (526 > 512). Truncating...\n",
      "WARNING:__main__:Text exceeded max tokens (522 > 512). Truncating...\n",
      "WARNING:__main__:Text exceeded max tokens (537 > 512). Truncating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Distribution (Top 20):\n",
      "NN: 167588\n",
      "JJ: 88004\n",
      "RB: 31871\n",
      "VBG: 23928\n",
      "VBD: 22437\n",
      "VBP: 15925\n",
      "VBN: 12850\n",
      "VB: 11634\n",
      "IN: 7483\n",
      "NNS: 6173\n",
      "CD: 3791\n",
      "VBZ: 2365\n",
      "MD: 2211\n",
      "JJR: 1928\n",
      "RBR: 1582\n",
      "JJS: 1349\n",
      "DT: 1343\n",
      "FW: 528\n",
      "PRP: 521\n",
      "CC: 486\n",
      "Bigram model successfully built, dataset processed, and output saved.\n"
     ]
    }
   ],
   "source": [
    "# File paths for your data folder\n",
    "base_folder = r\"C:\\Users\\laure\\Desktop\\dissertation_notebook\"\n",
    "data_folder = os.path.join(base_folder, \"Data\")\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "aggregated_path = os.path.join(data_folder, \"aggregated_raw_reddit_data.json\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for building bigram model, processing Reddit data, and saving the results.\n",
    "\n",
    "    Steps:\n",
    "    1. Reads the aggregated raw data from JSON.\n",
    "    2. Performs minimal tokenisation on titles, selftexts, and comments to build a bigram model.\n",
    "    3. Prints the top 20 bigrams found in the corpus, along with a POS distribution.\n",
    "    4. Uses RedditDataProcessor for thorough text preprocessing (artefact cleaning, \n",
    "       tokenisation, lemmatisation, bigram detection, etc.).\n",
    "    5. Saves the final processed data to 'lda_ready_data.json'.\n",
    "    Now preserves post_id and comment_id fields throughout processing.\n",
    "    \"\"\"\n",
    "    # Use aggregated_path as the input to the pipeline\n",
    "    input_path = Path(aggregated_path)\n",
    "    \n",
    "    # Output path in the same Data folder\n",
    "    output_path = Path(os.path.join(data_folder, \"lda_ready_data.json\"))\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Load raw aggregated data from JSON\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Build bigram model from minimal-tokenised data\n",
    "        all_texts = []\n",
    "        for post in data:\n",
    "            # Get IDs for logging purposes\n",
    "            post_id = post.get('post_id', 'unknown')\n",
    "            \n",
    "            raw_title = post.get('title', '')\n",
    "            raw_selftext = post.get('selftext', '')\n",
    "            combined_raw = f\"{raw_title} {raw_selftext}\"\n",
    "\n",
    "            # Tokenise the combined post text using minimal_tokenize\n",
    "            post_tokens = minimal_tokenize(combined_raw)\n",
    "            if post_tokens:\n",
    "                all_texts.append(post_tokens)\n",
    "\n",
    "            # Tokenise each comment if present\n",
    "            for c in post.get('comments', []):\n",
    "                comment_id = c.get('comment_id', 'unknown')\n",
    "                c_text = c.get('comment', '')\n",
    "                c_tokens = minimal_tokenize(c_text)\n",
    "                if c_tokens:\n",
    "                    all_texts.append(c_tokens)\n",
    "\n",
    "        # Create a global bigram model using Gensim Phrases & Phraser\n",
    "        global bigram_model\n",
    "        phrases = Phrases(all_texts, min_count=5, threshold=20)\n",
    "        bigram_model = Phraser(phrases)\n",
    "\n",
    "        # Display top 20 bigrams with their scores\n",
    "        bigram_freqs = bigram_model.phrasegrams\n",
    "        sorted_bigrams = sorted(bigram_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Top 20 Bigrams:\")\n",
    "        for i, (bg, score) in enumerate(sorted_bigrams[:20]):\n",
    "            print(f\"{i+1}. {bg} - Score: {score}\")\n",
    "\n",
    "        # Process the dataset using our RedditDataProcessor\n",
    "        processor = RedditDataProcessor(model_name=\"bert-base-uncased\")\n",
    "        processed_data = processor.process_dataset(data)\n",
    "\n",
    "        # Display top 20 part-of-speech tags encountered\n",
    "        global pos_distribution\n",
    "        print(\"\\nPOS Distribution (Top 20):\")\n",
    "        for tag, count in pos_distribution.most_common(20):\n",
    "            print(f\"{tag}: {count}\")\n",
    "\n",
    "        # Write out the final JSON result\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        logger.info(\"Successfully processed posts with threshold checks, POS tagging, bigrams, and re-ordered fields.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"Bigram model successfully built, dataset processed, and output saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b38bc-ab09-4701-b0f2-59c54dd22aba",
   "metadata": {},
   "source": [
    "## Concluson\n",
    "\n",
    "This notebook demonstrates a full end-to-end pipeline for preparing Reddit data for topic modelling and other NLP tasks. Patterns and stopwords are defined, bigram models are built and applied, and a series of text-processing steps are used to clean, lemmatise, and tokenise the raw data. By integrating these procedures into a coherent framework, an LDA-ready dataset is produced, preserving valuable linguistic features while removing extraneous noise. This final output, enhanced with part-of-speech tagging and bigrams, can then be employed for subsequent topic or text analysis.\n",
    "\n",
    "## References\n",
    "\n",
    "**Reference:**  \n",
    "Thomas, W., Debut, L., Sanh, V., et al. (2024) *Transformers v4.47.1* [computer program].  \n",
    "Available from: [https://huggingface.co/docs/transformers](https://huggingface.co/docs/transformers) [Accessed 25 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Transformers GitHub](https://github.com/huggingface/transformers)\n",
    "\n",
    "**Reference:**  \n",
    "Devlin, J., Chang, M. W., Lee, K., and Toutanova, K. (2018) *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*.  \n",
    "Available from: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805) [Accessed 28 May 2024].\n",
    "\n",
    "**Model:**  \n",
    "- [BERT-Base, Uncased](https://huggingface.co/bert-base-uncased)\n",
    "\n",
    "**Reference:**  \n",
    "Bird, S., Klein, E., and Loper, E. (2009) *Natural Language Processing with Python*. O'Reilly Media Inc.  \n",
    "Available from: [https://www.nltk.org/](https://www.nltk.org/) [Accessed 28 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [NLTK GitHub](https://github.com/nltk/nltk)\n",
    "\n",
    "**Reference:**  \n",
    "Řehůřek, R., and Sojka, P. (2010) *Software Framework for Topic Modelling with Large Corpora*. In *Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks*.  \n",
    "Available from: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/) [Accessed 3 June 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Gensim GitHub](https://github.com/RaRe-Technologies/gensim)\n",
    "\n",
    "**Reference:**  \n",
    "Pandas Development Team (2024) *pandas: Powerful data structures for data analysis v2.2.3* [computer program].  \n",
    "Available from: [https://pandas.pydata.org/](https://pandas.pydata.org/) [Accessed 11 May 2024].\n",
    "\n",
    "**Git Repo:**  \n",
    "- [Pandas GitHub](https://github.com/pandas-dev/pandas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_notebook Kernel",
   "language": "python",
   "name": "diss_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
